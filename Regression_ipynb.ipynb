{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xwq4iHFDF19-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is Simple Linear Regression?\n",
        "Simple Linear Regression is a statistical method used to model the relationship between two variables by fitting a linear equation to observed data. It predicts the value of a dependent variable (Y) based on the value of an independent variable (X) using the equation:\n",
        "\n",
        "ğ‘Œ\n",
        "=\n",
        "ğ‘š\n",
        "ğ‘‹\n",
        "+\n",
        "ğ‘\n",
        "Y=mX+c\n",
        "Where:\n",
        "\n",
        "ğ‘š\n",
        "m = slope of the line (indicates the change in\n",
        "ğ‘Œ\n",
        "Y for a unit change in\n",
        "ğ‘‹\n",
        "X)\n",
        "ğ‘\n",
        "c = y-intercept (the value of\n",
        "ğ‘Œ\n",
        "Y when\n",
        "ğ‘‹\n",
        "=\n",
        "0\n",
        "X=0)"
      ],
      "metadata": {
        "id": "Bz66JKaQGB23"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What are the key assumptions of Simple Linear Regression?\n",
        "Linearity: The relationship between the independent and dependent variables is linear.\n",
        "Independence: Observations are independent of each other.\n",
        "Homoscedasticity: Constant variance of residuals across all levels of\n",
        "ğ‘‹\n",
        "X.\n",
        "Normality: Residuals (errors) should be normally distributed.\n",
        "No multicollinearity: Only applies to multiple regression; in simple regression, there's just one predictor.\n",
        "3. What does the coefficient\n",
        "ğ‘š\n",
        "m represent in the equation\n",
        "ğ‘Œ\n",
        "=\n",
        "ğ‘š\n",
        "ğ‘‹\n",
        "+\n",
        "ğ‘\n",
        "Y=mX+c?\n",
        "The coefficient\n",
        "ğ‘š\n",
        "m (slope) represents the rate of change in the dependent variable\n",
        "ğ‘Œ\n",
        "Y for a one-unit increase in the independent variable\n",
        "ğ‘‹\n",
        "X. In other words, it indicates how much\n",
        "ğ‘Œ\n",
        "Y is expected to increase (or decrease if negative) for each additional unit of\n",
        "ğ‘‹\n",
        "X.\n",
        "\n",
        "4. What does the intercept\n",
        "ğ‘\n",
        "c represent in the equation\n",
        "ğ‘Œ\n",
        "=\n",
        "ğ‘š\n",
        "ğ‘‹\n",
        "+\n",
        "ğ‘\n",
        "Y=mX+c?\n",
        "The intercept\n",
        "ğ‘\n",
        "c is the point at which the regression line crosses the y-axis. It represents the predicted value of\n",
        "ğ‘Œ\n",
        "Y when\n",
        "ğ‘‹\n",
        "=\n",
        "0\n",
        "X=0.\n",
        "\n"
      ],
      "metadata": {
        "id": "s-dnyebTGDIc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. How do we calculate the slope\n",
        "ğ‘š\n",
        "m in Simple Linear Regression?\n",
        "The slope\n",
        "ğ‘š\n",
        "m is calculated using:\n",
        "\n",
        "ğ‘š\n",
        "=\n",
        "âˆ‘\n",
        "(\n",
        "ğ‘‹\n",
        "ğ‘–\n",
        "âˆ’\n",
        "ğ‘‹\n",
        "Ë‰\n",
        ")\n",
        "(\n",
        "ğ‘Œ\n",
        "ğ‘–\n",
        "âˆ’\n",
        "ğ‘Œ\n",
        "Ë‰\n",
        ")/\n",
        "âˆ‘\n",
        "(\n",
        "ğ‘‹\n",
        "ğ‘–\n",
        "âˆ’\n",
        "ğ‘‹\n",
        "Ë‰\n",
        ")\n",
        "2\n",
        "\n",
        "\n",
        "m=\n",
        "âˆ‘(X\n",
        "i\n",
        "â€‹\n",
        " âˆ’\n",
        "X\n",
        "Ë‰\n",
        " )\n",
        "2\n",
        "\n",
        "âˆ‘(X\n",
        "i\n",
        "â€‹\n",
        " âˆ’\n",
        "X\n",
        "Ë‰\n",
        " )(Y\n",
        "i\n",
        "â€‹\n",
        " âˆ’\n",
        "Y\n",
        "Ë‰\n",
        " )\n",
        "â€‹\n",
        "\n",
        "Where:\n",
        "\n",
        "ğ‘‹\n",
        "ğ‘–\n",
        ",\n",
        "ğ‘Œ\n",
        "ğ‘–\n",
        "X\n",
        "i\n",
        "â€‹\n",
        " ,Y\n",
        "i\n",
        "â€‹\n",
        "  = individual data points\n",
        "ğ‘‹\n",
        "Ë‰\n",
        ",\n",
        "ğ‘Œ\n",
        "Ë‰\n",
        "X\n",
        "Ë‰\n",
        " ,\n",
        "Y\n",
        "Ë‰\n",
        "  = means of\n",
        "ğ‘‹\n",
        "X and\n",
        "ğ‘Œ\n",
        "Y"
      ],
      "metadata": {
        "id": "urosb2Q6GJuW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "yjangcR1Gk9C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. What is the purpose of the least squares method in Simple Linear Regression?\n",
        "The least squares method aims to minimize the sum of squared residuals (errors) between observed values and predicted values. It finds the best-fitting line by reducing the differences between the actual data points and the predicted points on the line.\n",
        "\n",
        "7. How is the coefficient of determination (\n",
        "ğ‘…\n",
        "2\n",
        "R\n",
        "2\n",
        " ) interpreted in Simple Linear Regression?\n",
        "ğ‘…\n",
        "2\n",
        "R\n",
        "2\n",
        "  measures the proportion of variance in the dependent variable that is explained by the independent variable.\n",
        "\n",
        "Ranges from 0 to 1:\n",
        "ğ‘…\n",
        "2\n",
        "=\n",
        "0\n",
        "R\n",
        "2\n",
        " =0: The model explains none of the variability.\n",
        "ğ‘…\n",
        "2\n",
        "=\n",
        "1\n",
        "R\n",
        "2\n",
        " =1: The model explains all the variability.\n",
        "8. What is Multiple Linear Regression?\n",
        "Multiple Linear Regression is an extension of Simple Linear Regression that models the relationship between a dependent variable and two or more independent variables using the equation:\n",
        "\n",
        "ğ‘Œ\n",
        "=\n",
        "ğ‘\n",
        "0\n",
        "+\n",
        "ğ‘\n",
        "1\n",
        "ğ‘‹\n",
        "1\n",
        "+\n",
        "ğ‘\n",
        "2\n",
        "ğ‘‹\n",
        "2\n",
        "+\n",
        "â‹¯\n",
        "+\n",
        "ğ‘\n",
        "ğ‘›\n",
        "ğ‘‹\n",
        "ğ‘›\n",
        "Y=b\n",
        "0\n",
        "â€‹\n",
        " +b\n",
        "1\n",
        "â€‹\n",
        " X\n",
        "1\n",
        "â€‹\n",
        " +b\n",
        "2\n",
        "â€‹\n",
        " X\n",
        "2\n",
        "â€‹\n",
        " +â‹¯+b\n",
        "n\n",
        "â€‹\n",
        " X\n",
        "n\n",
        "â€‹\n",
        "\n",
        "Where\n",
        "ğ‘\n",
        "0\n",
        "b\n",
        "0\n",
        "â€‹\n",
        "  is the intercept and\n",
        "ğ‘\n",
        "ğ‘›\n",
        "b\n",
        "n\n",
        "â€‹\n",
        "  are the coefficients for each independent variable\n",
        "ğ‘‹\n",
        "ğ‘›\n",
        "X\n",
        "n\n",
        "â€‹\n",
        " .\n",
        "\n",
        "9. What is the main difference between Simple and Multiple Linear Regression?\n",
        "Simple Linear Regression: Involves one independent variable.\n",
        "Multiple Linear Regression: Involves two or more independent variables.\n",
        "10. What are the key assumptions of Multiple Linear Regression?\n",
        "Linearity: Relationship between dependent and independent variables is linear.\n",
        "Independence: Observations are independent.\n",
        "Homoscedasticity: Constant variance of residuals.\n",
        "Normality: Residuals should be normally distributed.\n",
        "No multicollinearity: Independent variables should not be highly correlated.\n",
        "11. What is heteroscedasticity, and how does it affect Multiple Linear Regression?\n",
        "Heteroscedasticity occurs when the variance of residuals is not constant across all levels of the independent variables. It can bias the standard errors, making hypothesis tests unreliable and affecting the accuracy of confidence intervals and significance tests.\n",
        "\n",
        "12. How can you improve a Multiple Linear Regression model with high multicollinearity?\n",
        "Remove highly correlated predictors.\n",
        "Combine correlated variables into a single feature.\n",
        "Use Principal Component Analysis (PCA).\n",
        "Apply regularization techniques like Ridge or Lasso regression.\n",
        "13. What are some common techniques for transforming categorical variables for regression models?\n",
        "One-Hot Encoding: Creates binary columns for each category.\n",
        "Label Encoding: Converts categories to integers.\n",
        "Target Encoding: Replaces categories with the mean of the target variable.\n",
        "Binary Encoding: Combines label and one-hot encoding for high cardinality features.\n",
        "14. What is the role of interaction terms in Multiple Linear Regression?\n",
        "Interaction terms capture the combined effect of two or more variables on the dependent variable. They help model situations where the impact of one predictor on the outcome depends on the level of another predictor.\n",
        "\n",
        "15. How can the interpretation of the intercept differ between Simple and Multiple Linear Regression?\n",
        "Simple Linear Regression: The intercept is the predicted value of\n",
        "ğ‘Œ\n",
        "Y when\n",
        "ğ‘‹\n",
        "=\n",
        "0\n",
        "X=0.\n",
        "Multiple Linear Regression: The intercept is the predicted value of\n",
        "ğ‘Œ\n",
        "Y when all independent variables are zero. Its interpretation might not always make sense, especially if zero is not a practical value for predictors.\n",
        "16. What is the significance of the slope in regression analysis, and how does it affect predictions?\n",
        "The slope indicates the rate of change in the dependent variable for a one-unit change in the independent variable. A positive slope suggests a direct relationship, while a negative slope suggests an inverse relationship. The magnitude of the slope reflects the strength of this effect.\n",
        "\n",
        "17. How does the intercept in a regression model provide context for the relationship between variables?\n",
        "The intercept provides a baseline value for the dependent variable when all predictors are zero. It helps in understanding the starting point of the relationship and makes predictions interpretable, especially when zero is a meaningful value for predictors."
      ],
      "metadata": {
        "id": "c2ZNcfJbGu-x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "3YxIAP81Gvwq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. What are the limitations of using\n",
        "ğ‘…\n",
        "2\n",
        "R\n",
        "2\n",
        "  as a sole measure of model performance?\n",
        "Doesn't account for overfitting:\n",
        "ğ‘…\n",
        "2\n",
        "R\n",
        "2\n",
        "  can increase by adding more predictors, even if they are not useful.\n",
        "No information about prediction accuracy: A high\n",
        "ğ‘…\n",
        "2\n",
        "R\n",
        "2\n",
        "  doesnâ€™t guarantee accurate predictions on new data.\n",
        "Ignores residual patterns:\n",
        "ğ‘…\n",
        "2\n",
        "R\n",
        "2\n",
        "  doesnâ€™t show if residuals have patterns indicating unmet assumptions.\n",
        "Not useful for nonlinear models: Can be misleading for models with non-linear relationships.\n",
        "Cannot detect multicollinearity: High\n",
        "ğ‘…\n",
        "2\n",
        "R\n",
        "2\n",
        "  doesnâ€™t mean variables are independent.\n",
        "19. How would you interpret a large standard error for a regression coefficient?\n",
        "A large standard error suggests that the estimated coefficient is unstable and may vary greatly with different samples. It indicates:\n",
        "\n",
        "Low precision: The true relationship between the predictor and the outcome is uncertain.\n",
        "Possible multicollinearity: High correlation among predictors can inflate standard errors.\n",
        "Insufficient data: A small sample size can cause large standard errors.\n",
        "20.. How can heteroscedasticity be identified in residual plots, and why is it important to address it?\n",
        "Identification:\n",
        "Plot residuals against predicted values.\n",
        "Heteroscedasticity is indicated by a â€œfan shapeâ€ or systematic pattern (increasing or decreasing spread).\n",
        "Importance:\n",
        "Violates regression assumptions.\n",
        "Biases standard errors: Leads to incorrect confidence intervals and p-values, affecting hypothesis testing.\n",
        "21. What does it mean if a Multiple Linear Regression model has a high\n",
        "ğ‘…\n",
        "2\n",
        "R\n",
        "2\n",
        "  but low adjusted\n",
        "ğ‘…\n",
        "2\n",
        "R\n",
        "2\n",
        " ?\n",
        "High\n",
        "ğ‘…\n",
        "2\n",
        "R\n",
        "2\n",
        "  indicates that a large portion of the variance is explained by the model.\n",
        "Low adjusted\n",
        "ğ‘…\n",
        "2\n",
        "R\n",
        "2\n",
        "  suggests that some predictors may not add value and could be noise, indicating overfitting.\n",
        "Adjusted\n",
        "ğ‘…\n",
        "2\n",
        "R\n",
        "2\n",
        "  penalizes for the number of predictors, providing a better measure of model quality.\n",
        "22. Why is it important to scale variables in Multiple Linear Regression?\n",
        "Improves numerical stability: Prevents some coefficients from dominating due to different scales.\n",
        "Facilitates interpretation: Scaled coefficients are comparable.\n",
        "Helps regularization techniques: Lasso and Ridge regressions require scaled data for effective penalty application.\n",
        "Gradient Descent efficiency: Scaled features speed up convergence in optimization algorithms.\n",
        "23. What is polynomial regression?\n",
        "Polynomial regression is an extension of linear regression that models a nonlinear relationship between the dependent and independent variables by introducing polynomial terms (squared, cubic, etc.). The model is still linear in terms of coefficients but allows for curvatures in the data.\n",
        "\n",
        "24.. How does polynomial regression differ from linear regression?\n",
        "Linear Regression: Fits a straight line (first-degree polynomial).\n",
        "Polynomial Regression: Fits a curved line by adding higher-degree polynomial terms, enabling it to capture non-linear patterns.\n",
        "Both are linear in terms of coefficients but differ in the nature of their predictors.\n",
        "25.. When is polynomial regression used?\n",
        "When data shows a nonlinear relationship that a straight line cannot capture.\n",
        "Situations with curved trends or local minima/maxima.\n",
        "Examples: growth curves, dose-response curves, and modeling complex phenomena.\n"
      ],
      "metadata": {
        "id": "Afa8zEN5Gvyh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "25.. What is the general equation for polynomial regression?\n",
        "ğ‘Œ\n",
        "=\n",
        "ğ‘\n",
        "0\n",
        "+\n",
        "ğ‘\n",
        "1\n",
        "ğ‘‹\n",
        "+\n",
        "ğ‘\n",
        "2\n",
        "ğ‘‹\n",
        "2\n",
        "+\n",
        "ğ‘\n",
        "3\n",
        "ğ‘‹\n",
        "3\n",
        "+\n",
        "â‹¯\n",
        "+\n",
        "ğ‘\n",
        "ğ‘›\n",
        "ğ‘‹\n",
        "ğ‘›\n",
        "+\n",
        "ğœ–\n",
        "Y=b\n",
        "0\n",
        "â€‹\n",
        " +b\n",
        "1\n",
        "â€‹\n",
        " X+b\n",
        "2\n",
        "â€‹\n",
        " X\n",
        "2\n",
        " +b\n",
        "3\n",
        "â€‹\n",
        " X\n",
        "3\n",
        " +â‹¯+b\n",
        "n\n",
        "â€‹\n",
        " X\n",
        "n\n",
        " +Ïµ\n",
        "Where:\n",
        "\n",
        "ğ‘\n",
        "0\n",
        ",\n",
        "ğ‘\n",
        "1\n",
        ",\n",
        "â€¦\n",
        ",\n",
        "ğ‘\n",
        "ğ‘›\n",
        "b\n",
        "0\n",
        "â€‹\n",
        " ,b\n",
        "1\n",
        "â€‹\n",
        " ,â€¦,b\n",
        "n\n",
        "â€‹\n",
        "  = coefficients\n",
        "ğ‘‹\n",
        ",\n",
        "ğ‘‹\n",
        "2\n",
        ",\n",
        "ğ‘‹\n",
        "3\n",
        ",\n",
        "â€¦\n",
        ",\n",
        "ğ‘‹\n",
        "ğ‘›\n",
        "X,X\n",
        "2\n",
        " ,X\n",
        "3\n",
        " ,â€¦,X\n",
        "n\n",
        "  = polynomial terms of the independent variable\n",
        "ğœ–\n",
        "Ïµ = error term\n",
        "26. Can polynomial regression be applied to multiple variables?\n",
        "Yes, it is called Polynomial Multiple Regression. The model includes interaction terms and polynomial terms for multiple predictors:\n",
        "\n",
        "ğ‘Œ\n",
        "=\n",
        "ğ‘\n",
        "0\n",
        "+\n",
        "ğ‘\n",
        "1\n",
        "ğ‘‹\n",
        "1\n",
        "+\n",
        "ğ‘\n",
        "2\n",
        "ğ‘‹\n",
        "1\n",
        "2\n",
        "+\n",
        "ğ‘\n",
        "3\n",
        "ğ‘‹\n",
        "2\n",
        "+\n",
        "ğ‘\n",
        "4\n",
        "ğ‘‹\n",
        "2\n",
        "2\n",
        "+\n",
        "ğ‘\n",
        "5\n",
        "ğ‘‹\n",
        "1\n",
        "ğ‘‹\n",
        "2\n",
        "+\n",
        "â‹¯\n",
        "+\n",
        "ğœ–\n",
        "Y=b\n",
        "0\n",
        "â€‹\n",
        " +b\n",
        "1\n",
        "â€‹\n",
        " X\n",
        "1\n",
        "â€‹\n",
        " +b\n",
        "2\n",
        "â€‹\n",
        " X\n",
        "1\n",
        "2\n",
        "â€‹\n",
        " +b\n",
        "3\n",
        "â€‹\n",
        " X\n",
        "2\n",
        "â€‹\n",
        " +b\n",
        "4\n",
        "â€‹\n",
        " X\n",
        "2\n",
        "2\n",
        "â€‹\n",
        " +b\n",
        "5\n",
        "â€‹\n",
        " X\n",
        "1\n",
        "â€‹\n",
        " X\n",
        "2\n",
        "â€‹\n",
        " +â‹¯+Ïµ\n",
        "This approach captures complex relationships among multiple variables.\n",
        "\n",
        "27. What are the limitations of polynomial regression?\n",
        "Overfitting: High-degree polynomials can fit noise in the data, reducing generalizability.\n",
        "Interpretability: Coefficients become hard to interpret with higher degrees.\n",
        "Extrapolation risk: Predictions outside the observed range can be highly inaccurate.\n",
        "Computational cost: More terms increase complexity and computational time.\n",
        "28. What methods can be used to evaluate model fit when selecting the degree of a polynomial?\n",
        "Cross-validation: Prevents overfitting by evaluating model performance on unseen data.\n",
        "Adjusted\n",
        "ğ‘…\n",
        "2\n",
        "R\n",
        "2\n",
        " : Penalizes complexity by adjusting\n",
        "ğ‘…\n",
        "2\n",
        "R\n",
        "2\n",
        " .\n",
        "AIC/BIC (Akaike/Bayesian Information Criterion): Penalizes models with too many parameters.\n",
        "Residual analysis: Check if residuals are randomly distributed.\n",
        "Mean Squared Error (MSE): Assesses average squared difference between actual and predicted values.\n",
        "29.. Why is visualization important in polynomial regression?\n",
        "Detects non-linearity: Helps verify if a polynomial fit is justified.\n",
        "Assumption checking: Residual plots can reveal issues like heteroscedasticity or autocorrelation.\n",
        "Degree selection: Visual comparison of different polynomial degrees assists in avoiding overfitting or underfitting.\n"
      ],
      "metadata": {
        "id": "Iz-ecfjiHK3a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "30.. How is polynomial regression implemented in Python?\n",
        "Libraries: numpy for data manipulation, sklearn for model building.\n",
        "Steps:\n",
        "Transform data: Use PolynomialFeatures from sklearn.preprocessing to create polynomial terms.\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "poly = PolynomialFeatures(degree=3)\n",
        "X_poly = poly.fit_transform(X)\n",
        "Fit model: Use LinearRegression to fit the transformed data.\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "from sklearn.linear_model import LinearRegression\n",
        "model = LinearRegression()\n",
        "model.fit(X_poly, y)\n",
        "Prediction:\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "y_pred = model.predict(X_poly)\n",
        "Evaluation: Use metrics like\n",
        "ğ‘…\n",
        "2\n",
        "R\n",
        "2\n",
        "  and MSE to assess performance."
      ],
      "metadata": {
        "id": "VDUMy4xqHgkG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "@0.How can heteroscedasticity be identified in residual plots?\n",
        "Heteroscedasticity occurs when the variance of residuals (errors) is not constant across all levels of the independent variables. You can identify it using a residual plot, which is a scatter plot of residuals (errors) on the y-axis against predicted values or an independent variable on the x-axis.\n",
        "\n",
        "Key indicators of heteroscedasticity in residual plots:\n",
        "\n",
        "Fan or cone shape: If the spread of residuals increases or decreases systematically as you move along the x-axis, this suggests heteroscedasticity.\n",
        "Patterns: Any clear pattern (like curves, U-shapes, or systematic waves) indicates a problem. Residuals should be randomly scattered if homoscedasticity (constant variance) holds.\n",
        "Residual vs. Fitted plot: Plotting residuals against fitted (predicted) values is a common method. A good plot should show no systematic patterns and have a uniform spread.\n",
        "Example in Python:\n",
        "\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming y_pred are predictions and residuals are calculated\n",
        "residuals = y - y_pred\n",
        "\n",
        "plt.scatter(y_pred, residuals)\n",
        "plt.axhline(y=0, color='red', linestyle='--')\n",
        "plt.title('Residuals vs Fitted')\n",
        "plt.xlabel('Fitted values')\n",
        "plt.ylabel('Residuals')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rkaG-xK4HrF-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "fORLbo6eGPSE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "H6BxXXYWF26g"
      }
    }
  ]
}